{
  
    
        "post0": {
            "title": "GD (Gradient Descent)",
            "content": "GD is the key that allow us to have a model that can get better and better and look for that perfection. For this we need a way to adjust the parameters so that we can get a better performance from each iteration. . We could look at each individual feature and come up with a set of parameters for each one, such that the highest parameters are associated with those features most likely to be important for a particular output. . This can be represented as a function and set of parameter values for each possible output instance the probability of being correct: . x= features . p=parameters . def pr_eight(x,p) = (x*p).sum() . x is represented as a vector, with all of the rows stacked up end to end into a single long line (x=[2,3,2,4,3,4,5,6,....,n]) and p is also a vector. If we have this function we only need a way of updating those &quot;p&quot; values until they are good as we can make them. . To be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier: . Initialize the weights. | For each feature, use these weights to predict the output. | Based on these predictions, calculate how good the model is (its loss). | Calculate the gradient, which measures for each weight, how changing that weight would change the loss | Step (that is, change) all the weights based on that calculation. | Go back to the step 2, and repeat the process. | Iterate until you decide to stop the training process (for instance, because the model is good enough or you don&#39;t want to wait any longer). | Initialize: We initialize the parameters to random values | Loss: testing the effectiveness of any current parameter assigment in terms of the actual performance. We need number that will return a small number if the performance was good or a large one if the performance was bad. | Step: A simple way to figure out whether a weight should be increased or decrease a bit. The best way to do this is by calculating the &quot;gradients&quot;. | Stop: Once you decided how many epochs (iterations) to train the model for, we apply that decision. Train until we ran out of time or the accuracy of the model starts to get worst. | . Simple case . from fastai.vision.all import * from fastbook import * . def f(x): return x**2 . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) plt.scatter(-1.5, f(-1.5), color=&#39;red&#39;); . If we decide to increment x just for a tiny value we can see that we would descend from the actual spot . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) plt.scatter(-1.5, f(-1.5), color=&#39;red&#39;, alpha=0.5); plt.scatter(-1, f(-1), color=&#39;red&#39;); . We try and get even lower . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) plt.scatter(-1.5, f(-1.5), color=&#39;red&#39;, alpha=0.5); plt.scatter(-1, f(-1), color=&#39;red&#39;, alpha=0.5); plt.scatter(-0.8, f(-0.8), color=&#39;red&#39;, alpha=0.5); plt.scatter(-0.7, f(-0.7), color=&#39;red&#39;); . Calculating the gradient . The &quot;one magic step&quot; is the bit where we calculate the gradients. As we mentioned, we use calculus as a performance optimization; it allows us to more quickly calculate whether our loss will go up or down when we adjust our parameters up or down. In other words, the gradients will tell us how much we have to change each weight to make our model better. . Did you study calculus in school? . If you remember that the derivative of a function tells you how much a change in its parameter will change its result. If not dont worry you just stop for a minute and go and watch this awesome video made by 3blue1brown . https://www.youtube.com/watch?v=9vKqVkMQHKk&amp;list=PL0-GT3co4r2wlh6UHTUeQsrf3mlS2lk6x&amp;index=2 . Now that you refresh about derivatives we can continue . Remember the function x^2? Well its derivative is another function that calculates the change, rather than de value. For instance, the derivative of x^2 at the value 5 tells us how rapidly the function changes at the value 5. When we know how our function will change, then we know what to do to make it smaller. This is the key to machine learning: having a way to change the parameters of a function to make it smaller. . One important thing to be aware of is that our function has lots of weights that we need to adjust, so when we calculate the derivative we won&#39;t get back one number, but lots of them a gradient for every weight. But there is nothing mathematically tricky here; you can calculate the derivative with respect to one weight, and treat all the other ones as constant, then repeat that for each other weight. This is how all of the gradients are calculated, for every weight. . Well... the best of all of this is that...PyTorch is able to automatically compute the derivative of nearly any function! and its surprinsingly fast . 1) Lests pick a tensor value which we want gradients at: . xt = tensor(5.).requires_grad_() . requiresgrad is a method brought to us by pytorch. We use it to tell Pytorch that we want to calculate grandients with respect to that variable at that specific value. This will make Pytorch remember to keep track of how to compute grandients of the other. . Now lets calculate the function with that specific value . yt = f(xt) yt . tensor(25., grad_fn=&lt;PowBackward0&gt;) . Finally we tell Pytorch to calculate the gradient for us . yt.backward() . xt.grad . tensor(10.) . If you remember your high school calculus rules, the derivative of x*2 is 2x, and we have x=3, so the gradients should be 2*5=10 which is what PyTorch calculated for us! . Lets now do it with a vector instead of only 1 number . xt = tensor([3., 5., 15.]).requires_grad_() xt . tensor([ 3., 5., 15.], requires_grad=True) . lets change the first function to add all those numbers in the vector . def f(x): return (x**2).sum() yt = f(xt) yt . tensor(259., grad_fn=&lt;SumBackward0&gt;) . yt.backward() xt.grad . tensor([ 6., 10., 30.]) . The gradients only tell us the slope of our function, they don&#39;t actually tell us exactly how far to adjust the parameters. But it gives us some idea of how far; if the slope is very large, then that may suggest that we have more adjustments to do, whereas if the slope is very small, that may suggest that we are close to the optimal value. . Adjusting using the Learning rate . Deciding how to modify our parameters based on the values of the gradients is a crusial part of the process of deep learning . We will multiply the gradient by some small number aka &quot;the learning rate (LR)&quot;. . Common pick numbers rank between 0.001 and 0.1. Once you have picked a LR, you can adjust your parameters using this simple funtion: . p -= gradient(p) * lr -&gt; this is known as stepping your parameters. . What happends if you pick a learning rate to small? . It can mean having to do a lot of steps :( . What happends if you pick a learning rate to high? . Well it could actually result in the loss getting worse and bouncing back up . Lets work on a end-to-end simple example . Lets take a look at GD and see how finding a minimum can be used to train a model to fit data better . Let&#39;s start with a simple model. . Imagine you were measuring the speed of a roller coaster as it went over the top of a hump. It would start fast, and then get slower as it went up the hill; it would be slowest at the top, and it would then speed up again as it went downhill. You want to build a model of how the speed changes over time. If you were measuring the speed manually every second for 60 seconds, it might look something like this: . time = torch.arange(0,20).float() . time . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) . speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1 . plt.scatter(time, speed); . Lets try and guess that is a &quot;quadratic function&quot; of the form: . a(time**2)+(btime)+c . def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . This greatly simplifies the problem, since every quadratic function is fully defined by the three parameters a, b, and c. Thus, to find the best quadratic function, we only need to find the best values for a, b, and c. . We need to define first what we mean by &quot;best.&quot; We define this precisely by choosing a loss function, which will return a value based on a prediction and a target, where lower values of the function correspond to &quot;better&quot; predictions. For continuous data, it&#39;s common to use mean squared error: . def mse(preds, targets): return ((preds-targets)**2).mean() . Now lets implement the 7 step process from the begining of the post . Step 1: Initialize the parameters . We are going to initialized each parameter with a random value and tell Pytorch that we want to track their gradients using _requiresgrad() . params = torch.randn(3).requires_grad_() . We can clone the original parameters to have them just in case . original_parameters = params.clone() . Step 2: Calculate the predictions . preds = f(time,params) . Lets see how the predictions are to our real targets . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) ax.set_ylim(-300,100) . show_preds(preds) . Wow! Terrible our random values think that the roller coster is going backwards... look at the negative speed . Can we do a better job? . Well, lets calculate the loss . Step 3: Calculate the loss . loss = mse(preds, speed) . loss . tensor(31688.4062, grad_fn=&lt;MeanBackward0&gt;) . Our goal is now to improve this. To do that, we&#39;ll need to know the gradients. . Step 4: Calculate the gradients . loss.backward() params.grad . tensor([-59061.2148, -3802.9961, -281.0778]) . We can now pick a learning rate to try and adjust this gradients . lr = 0.00001 params.data -= lr * params.grad.data params.grad = None . Lets see if the loss has improved: . preds = f(time,params) mse(preds, speed) . tensor(6555.9229, grad_fn=&lt;MeanBackward0&gt;) . show_preds(preds) . We need to repeat this a few times . def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . Step 6: Repeat the process . Now we repeat this process a bunch of times and see if we get any improvements . for i in range(20): apply_step(params) . 6555.9228515625 1800.084228515625 900.1290283203125 729.8250732421875 697.5931396484375 691.4888305664062 690.3287353515625 690.1041870117188 690.0565795898438 690.0423583984375 690.0346069335938 690.0282592773438 690.0219116210938 690.015625 690.0093383789062 690.0029907226562 689.9968872070312 689.9906005859375 689.9844360351562 689.9781494140625 . # this time with a graph params = original_parameters.detach().requires_grad_() . _,axs = plt.subplots(1,4,figsize=(12,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . Step 7: stop . Just decide a number of epochs and stop the process when you feel happy with the loss . I hope that this post help you undestand a bit more on why we use gradient Descent in machine learning and how powerful it can be. In my next post we will discuss this process in more detail using the Mnist dataset as example to show you GD apply into a real problem. .",
            "url": "https://juanchoalric.github.io/ai_journey/deep-learning/jupyter/fastai/2020/09/30/Gradient-descent-simple-example.html",
            "relUrl": "/deep-learning/jupyter/fastai/2020/09/30/Gradient-descent-simple-example.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Creating a Simpler Model",
            "content": "# your machine to run the code !pip install -Uqq fastbook import fastbook fastbook.setup_book() . voila 0.2.3 has requirement nbconvert&lt;7,&gt;=6.0.0, but you&#39;ll have nbconvert 5.6.1 which is incompatible. . /home/juancruzalric/.pyenv/versions/3.7.4/envs/fastai/lib/python3.7/site-packages/fastbook/__init__.py:22: UserWarning: Missing `graphviz` - please run `conda install fastbook` except ModuleNotFoundError: warn(&#34;Missing `graphviz` - please run `conda install fastbook`&#34;) . from fastai.vision.all import * from fastbook import * # We are going to work with grey scale images matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . Mnist Dataset . Mnist Dataset contains a lot of images from 0 to 9. We are going to try and classify any image as a 3 or a 7 . URLs.MNIST_SAMPLE . &#39;https://s3.amazonaws.com/fast-ai-sample/mnist_sample.tgz&#39; . path = untar_data(URLs.MNIST_SAMPLE) . path . Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample&#39;) . path.ls() . (#3) [Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/valid&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/labels.csv&#39;)] . ls() &gt; is a method created by fastai. It let you see what is inside the directory. It will return a object called by &quot;L&quot;. It has the same functionality as a &quot;list&quot; in python. . We can also see that we have a train directory and a valid directory . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/7&#39;)] . There are two directories inside &quot;/train&quot; one is &quot;/train/7&quot; and the other one is &quot;/train/3&quot;. The &quot;/3&quot; and &quot;/7&quot; represents the labels of the data . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() threes . (#6131) [Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10000.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10011.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10031.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10034.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10042.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10052.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/1007.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10074.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10091.png&#39;)...] . Lets try and print out one of the images . img3_path = threes[1] im3 = Image.open(img3_path) im3 . img7_path = sevens[1] im7 = Image.open(img7_path) im7 . But sadly... a computer only undestands numbers we will need to transform this images . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . We are printing rows from 4 up to 10 (not including 10) and columns 4 to 10 (not including 10) . We can see it more clearly using a Dataframe (Pandas library). . type(array(im3)) . numpy.ndarray . type(tensor(im3)) . torch.Tensor . tensors and numpy arrays are similar. However tensors let you run your operations in the GPU making your work process faster. We are going to use tensors from now on. . img3_t = tensor(im3) df = pd.DataFrame(img3_t[4:22, 4:22]) ## Just setting the font-size and a background color as Grey with some gradient to see the ## density of the pixels df.style.set_properties(**{&#39;font-size&#39;:&#39;10pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . 11 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 9 | 51 | 119 | 253 | 253 | 253 | 76 | 0 | . 12 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 183 | 253 | 253 | 139 | 0 | . 13 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 182 | 253 | 253 | 104 | 0 | . 14 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 85 | 249 | 253 | 253 | 36 | 0 | . 15 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 60 | 214 | 253 | 253 | 173 | 11 | 0 | . 16 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 98 | 247 | 253 | 253 | 226 | 9 | 0 | 0 | . 17 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 42 | 150 | 252 | 253 | 253 | 233 | 53 | 0 | 0 | 0 | . Creating a SIMPLE Model . We are going to create a simple model that takes the AVG (averages) of each class to predict and use that to check if a new image pixels are near the average of the 3 or the 7. . seven_tensors = [tensor(Image.open(path)) for path in sevens] three_tensors = [tensor(Image.open(path)) for path in threes] len(three_tensors), len(seven_tensors) . (6131, 6265) . This is getting each path for the corresponding images and transforming them into a tensor, however all those tensors are inside a list . type(seven_tensors) . list . type(seven_tensors[0]) . torch.Tensor . show_image(seven_tensors[5]); . show_image() is going to transform those tensors into a image because each value inside the tensor represent a pixel . We would like to compute the avg of the density of each pixel. We will neet to stack each image and create a 3 dimencional tensor, which the hight is going to be the amount of images, and the other 2 dimensions are going to be 28x28 pixels size. We can use the stack method given us by pytorch. . stacked_sevens = torch.stack(seven_tensors).float() / 255 stacked_threes = torch.stack(three_tensors).float() / 255 stacked_threes.shape . torch.Size([6131, 28, 28]) . we can also check the dimensions by calling ndim . stacked_threes.ndim . 3 . mean3 = stacked_threes.mean(0) show_image(mean3) . &lt;AxesSubplot:&gt; . mean7 = stacked_sevens.mean(0) show_image(mean7) . &lt;AxesSubplot:&gt; . Now we have the ideal digits for 3 and 7. Let us pick an arbitrary 3 and measure its distance to the ideal one. We are going to explain two different ways . Take the mean of the absolute value of differences (absolute value is the function that replaces negative values with positive values). This is called the mean absolute difference or L1 norm | Take the mean of the square of differences (which makes everything positive) and then take the square root (which undoes the squaring). This is called the root mean squared error (RMSE) or L2 norm. | . a_3 = stacked_threes[3] show_image(a_3); . dist_3_abs = (a_3 - mean3).abs().mean() dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt() dist_3_abs, dist_3_sqr . (tensor(0.1154), tensor(0.2064)) . dist_7_abs = (a_3 - mean7).abs().mean() dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt() dist_7_abs, dist_7_sqr . (tensor(0.1669), tensor(0.3170)) . We can see that the distance betweem the 3 and the &quot;ideal&quot; 3 is shorter than the &quot;ideal&quot; 7 pytorch already provides both of these as loss functions . F.l1_loss(a_3.float(), mean7), F.mse_loss(a_3, mean7).sqrt() . (tensor(0.1669), tensor(0.3170)) . The mean_square_error will penalize bigger mistakes more heavily and be more lenient with small mistakes . Metric . A metric is a number that is calculated based on the predictions of our model, and the correct labels in our dataset, in order to tell us how good our model is. Remeber that the Mnist dataset already has a directory called &quot;Valid&quot;. This directory contains the data where we are going to calculate how well our model operates. . We are going to use the &#39;stack&#39; pytorch operation to stack all the images contain in de valid directory . valid_3_tens = torch.stack([tensor(Image.open(path)) for path in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(path)) for path in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Here we see two tensors, one representing the 3s validation set of 1,010 images of size 28×28, and one representing the 7s validation set of 1,028 images of size 28×28. . We can write a simple function that calculates the mean absolute error using an experssion very similar to the one we wrote in the last section: . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) . valid_3_dist = mnist_distance(valid_3_tens, mean3) valid_3_dist, valid_3_dist.shape . (tensor([0.1605, 0.1107, 0.1181, ..., 0.1419, 0.1103, 0.1162]), torch.Size([1010])) . We can use mnist_distance to figure out whether an image is a 3 or not by using the following logic: if the distance between the digit in question and the ideal 3 is less than the distance to the ideal 7, then it&#39;s a 3. This function will automatically do broadcasting and be applied elementwise, just like all PyTorch functions and operators: . def is_3(x): return mnist_distance(x, mean3) &lt; mnist_distance(x, mean7) . Let&#39;s test it with one of our examples . is_3(a_3), is_3(a_3).float() . (tensor(True), tensor(1.)) . is_3(valid_3_tens) . tensor([False, True, True, ..., True, True, True]) . Now we can calculate the accuracy for each of the 3s and 7s by taking the average of that function for all 3s and its inverse for all 7s: . accuracy_3s = is_3(valid_3_tens).float().mean() accuracy_7s = (1- is_3(valid_7_tens).float()).mean() accuracy_3s, accuracy_7s, (accuracy_3s+accuracy_7s)/2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) . We&#39;re getting over 90% accuracy on both 3s and 7s, and we&#39;ve seen how to define a metric conveniently using broadcasting. . However: 3s and 7s are very different-looking digits. And we&#39;re only classifying 2 out of the 10 possible digits so far. So we&#39;re going to need to do better! . But now we know a simpler model that can predict with a 95% accuracy and if we create a deep learning model that predict with 94% accuracy we can clearly see that is not a good model and we will need to try other things. .",
            "url": "https://juanchoalric.github.io/ai_journey/deep-learning/jupyter/fastai/2020/09/28/creating-simple-model.html",
            "relUrl": "/deep-learning/jupyter/fastai/2020/09/28/creating-simple-model.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastai First Chapter",
            "content": "Why is it hard to use a traditional computer program to recognize images in a photo? . We are going to answer this question by giving an example: Imagine that you make a program that tells you if the image that you pass is an apple . . But not every apple has the same size and color... . . Image creating a bunch of manual rules for only one image just to get rekt by the second image they give you. . And that&#39;s why machine learning is perfect for solving image detection problems, just let the computer create her own rules that work for any apple image. . What did the author mean by &quot;weight assignment&quot;? . Weight assignments are a specific way to add values to the current weights. In deep learning we use Parameters to refer to &quot;weights&quot; . What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy? . The universal approximation theorem shows that this function can solve any problem to any level of accuracy, in theory. . Neural network diagram . . What do you need in order to train a model? . You need DATA. If you are working in a supervise problem you also need the labels for that particula dataset . Do we always have to use 224&#215;224-pixel images with the cat recognition model? . No, 224x224 is a well establish size known by researchers, but you can use the size you want. Just take into account that if you increase the size you may get better predictions because the model can track more data. However, you will lose speed in training your model. . What is the difference between classification and regression? . The difference between classification and regression is the target you want to predict. In REGRESSION problems you want to predict a number (ex:123). In CLASSIFICATION problems you want to predict a specific class (ex: if its a dog or a cat) . What is a validation set? What is a test set? Why do we need them? . A validation set is used to tell if our model is performing better in each iteration in the training face. The test set is only used when we are completely sure that we are going to use that model and we are ready to deploy de model to production. . What will fastai do if you don&#39;t provide a validation set? . It will create a validation set for you . What is overfitting? Provide an example. . Overfitting is when your model memorizes all your training set so well that when given new unseen data it gives a terrible result. Example: Image a teacher tells you to only study from these questions for the exam. However, when the day of the exam arrives the test is completely different. You study only the questions by hard but you only can answer exactly those. If you are given another set of questions you would perform poorly. . What is a metric? How does it differ from &quot;loss&quot;? . A metric is a measurement of how good the model is, using the validation set, chosen for human perception. However, the loss is also a measure of how good the model is, chosen to drive training via SGD and in charge of modifying the parameters. . How can pretrained models help? . Transfer learning models help us because they already have their weights already initialized. It will also reduce the amount of time we are going to need our model to train and also the amount of data that we need to have. . What is the &quot;head&quot; of a model? . The head of a model is the part that is newly added to be specific to the new dataset. . What is an &quot;architecture&quot;? . An &quot;architecture&quot; is the actual mathematical function that we&#39;re passing the input data and parameters to . What is segmentation? . Creating a model that can recognize the content of every individual pixel in an image is called segmentation. . What is y_range used for? When do we need it? . Y_range is used when you&#39;re predicting a continuous number, rather than a category, so we have to tell fastai what range our target has, using the y_range parameter. . Summary . The first chapter was a perfect and clear introduction to a lot of words that we use all-day in Data Science. Hopefully, in the following chapters, we can get more deeply into these subjects. .",
            "url": "https://juanchoalric.github.io/ai_journey/deep-learning/jupyter/fastai/2020/09/26/fastai-first-questioner.html",
            "relUrl": "/deep-learning/jupyter/fastai/2020/09/26/fastai-first-questioner.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://juanchoalric.github.io/ai_journey/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://juanchoalric.github.io/ai_journey/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://juanchoalric.github.io/ai_journey/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}