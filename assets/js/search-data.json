{
  
    
        "post0": {
            "title": "Building your first model in keras and get it ready for mobile use",
            "content": "import tensorflow as tf from tensorflow.keras.models import Model from tensorflow.keras.layers import Input import pathlib import matplotlib.pyplot as plt import numpy as np . We are going to create a basic model of the form y=mx+b . x = [] for i in range(-5,10): x.append(i) . x . [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . y = [] for i in range(len(x)): y.append((x[i] * 2) - 1) . y . [-11, -9, -7, -5, -3, -1, 1, 3, 5, 7, 9, 11, 13, 15, 17] . model = tf.keras.Sequential([ tf.keras.layers.Dense(units=1, input_shape=[1]) ]) . model.compile(optimizer=&#39;sgd&#39;, loss=&#39;mean_squared_error&#39;, metrics=[&#39;mse&#39;]) . model.fit( x=x, y=y, epochs=200 ) . Epoch 1/200 1/1 [==============================] - 0s 1ms/step - loss: 231.3291 - mse: 231.3291 Epoch 2/200 1/1 [==============================] - 0s 2ms/step - loss: 69.1295 - mse: 69.1295 Epoch 3/200 1/1 [==============================] - 0s 1ms/step - loss: 21.2743 - mse: 21.2743 Epoch 4/200 1/1 [==============================] - 0s 3ms/step - loss: 7.1351 - mse: 7.1351 Epoch 5/200 1/1 [==============================] - 0s 1ms/step - loss: 2.9382 - mse: 2.9382 Epoch 6/200 1/1 [==============================] - 0s 2ms/step - loss: 1.6736 - mse: 1.6736 Epoch 7/200 1/1 [==============================] - 0s 3ms/step - loss: 1.2746 - mse: 1.2746 Epoch 8/200 1/1 [==============================] - 0s 3ms/step - loss: 1.1317 - mse: 1.1317 Epoch 9/200 1/1 [==============================] - 0s 4ms/step - loss: 1.0650 - mse: 1.0650 Epoch 10/200 1/1 [==============================] - 0s 2ms/step - loss: 1.0217 - mse: 1.0217 Epoch 11/200 1/1 [==============================] - 0s 3ms/step - loss: 0.9859 - mse: 0.9859 Epoch 12/200 1/1 [==============================] - 0s 2ms/step - loss: 0.9532 - mse: 0.9532 Epoch 13/200 1/1 [==============================] - 0s 2ms/step - loss: 0.9221 - mse: 0.9221 Epoch 14/200 1/1 [==============================] - 0s 2ms/step - loss: 0.8921 - mse: 0.8921 Epoch 15/200 1/1 [==============================] - 0s 2ms/step - loss: 0.8632 - mse: 0.8632 Epoch 16/200 1/1 [==============================] - 0s 2ms/step - loss: 0.8352 - mse: 0.8352 Epoch 17/200 1/1 [==============================] - 0s 2ms/step - loss: 0.8081 - mse: 0.8081 Epoch 18/200 1/1 [==============================] - 0s 3ms/step - loss: 0.7820 - mse: 0.7820 Epoch 19/200 1/1 [==============================] - 0s 2ms/step - loss: 0.7566 - mse: 0.7566 Epoch 20/200 1/1 [==============================] - 0s 3ms/step - loss: 0.7321 - mse: 0.7321 Epoch 21/200 1/1 [==============================] - 0s 2ms/step - loss: 0.7084 - mse: 0.7084 Epoch 22/200 1/1 [==============================] - 0s 1ms/step - loss: 0.6854 - mse: 0.6854 Epoch 23/200 1/1 [==============================] - 0s 1ms/step - loss: 0.6632 - mse: 0.6632 Epoch 24/200 1/1 [==============================] - 0s 5ms/step - loss: 0.6417 - mse: 0.6417 Epoch 25/200 1/1 [==============================] - 0s 2ms/step - loss: 0.6209 - mse: 0.6209 Epoch 26/200 1/1 [==============================] - 0s 2ms/step - loss: 0.6008 - mse: 0.6008 Epoch 27/200 1/1 [==============================] - 0s 5ms/step - loss: 0.5813 - mse: 0.5813 Epoch 28/200 1/1 [==============================] - 0s 1ms/step - loss: 0.5625 - mse: 0.5625 Epoch 29/200 1/1 [==============================] - 0s 2ms/step - loss: 0.5442 - mse: 0.5442 Epoch 30/200 1/1 [==============================] - 0s 4ms/step - loss: 0.5266 - mse: 0.5266 Epoch 31/200 1/1 [==============================] - 0s 3ms/step - loss: 0.5095 - mse: 0.5095 Epoch 32/200 1/1 [==============================] - 0s 8ms/step - loss: 0.4930 - mse: 0.4930 Epoch 33/200 1/1 [==============================] - 0s 4ms/step - loss: 0.4770 - mse: 0.4770 Epoch 34/200 1/1 [==============================] - 0s 3ms/step - loss: 0.4616 - mse: 0.4616 Epoch 35/200 1/1 [==============================] - 0s 2ms/step - loss: 0.4466 - mse: 0.4466 Epoch 36/200 1/1 [==============================] - 0s 5ms/step - loss: 0.4321 - mse: 0.4321 Epoch 37/200 1/1 [==============================] - 0s 2ms/step - loss: 0.4181 - mse: 0.4181 Epoch 38/200 1/1 [==============================] - 0s 2ms/step - loss: 0.4046 - mse: 0.4046 Epoch 39/200 1/1 [==============================] - 0s 2ms/step - loss: 0.3915 - mse: 0.3915 Epoch 40/200 1/1 [==============================] - 0s 1ms/step - loss: 0.3788 - mse: 0.3788 Epoch 41/200 1/1 [==============================] - 0s 2ms/step - loss: 0.3665 - mse: 0.3665 Epoch 42/200 1/1 [==============================] - 0s 2ms/step - loss: 0.3546 - mse: 0.3546 Epoch 43/200 1/1 [==============================] - 0s 4ms/step - loss: 0.3431 - mse: 0.3431 Epoch 44/200 1/1 [==============================] - 0s 2ms/step - loss: 0.3320 - mse: 0.3320 Epoch 45/200 1/1 [==============================] - 0s 2ms/step - loss: 0.3213 - mse: 0.3213 Epoch 46/200 1/1 [==============================] - 0s 1ms/step - loss: 0.3108 - mse: 0.3108 Epoch 47/200 1/1 [==============================] - 0s 1ms/step - loss: 0.3008 - mse: 0.3008 Epoch 48/200 1/1 [==============================] - ETA: 0s - loss: 0.2910 - mse: 0.291 - 0s 5ms/step - loss: 0.2910 - mse: 0.2910 Epoch 49/200 1/1 [==============================] - 0s 10ms/step - loss: 0.2816 - mse: 0.2816 Epoch 50/200 1/1 [==============================] - 0s 3ms/step - loss: 0.2725 - mse: 0.2725 Epoch 51/200 1/1 [==============================] - 0s 2ms/step - loss: 0.2636 - mse: 0.2636 Epoch 52/200 1/1 [==============================] - 0s 2ms/step - loss: 0.2551 - mse: 0.2551 Epoch 53/200 1/1 [==============================] - 0s 2ms/step - loss: 0.2468 - mse: 0.2468 Epoch 54/200 1/1 [==============================] - 0s 3ms/step - loss: 0.2388 - mse: 0.2388 Epoch 55/200 1/1 [==============================] - 0s 3ms/step - loss: 0.2311 - mse: 0.2311 Epoch 56/200 1/1 [==============================] - 0s 2ms/step - loss: 0.2236 - mse: 0.2236 Epoch 57/200 1/1 [==============================] - 0s 4ms/step - loss: 0.2163 - mse: 0.2163 Epoch 58/200 1/1 [==============================] - 0s 7ms/step - loss: 0.2093 - mse: 0.2093 Epoch 59/200 1/1 [==============================] - 0s 4ms/step - loss: 0.2025 - mse: 0.2025 Epoch 60/200 1/1 [==============================] - 0s 3ms/step - loss: 0.1960 - mse: 0.1960 Epoch 61/200 1/1 [==============================] - 0s 2ms/step - loss: 0.1896 - mse: 0.1896 Epoch 62/200 1/1 [==============================] - 0s 8ms/step - loss: 0.1835 - mse: 0.1835 Epoch 63/200 1/1 [==============================] - 0s 4ms/step - loss: 0.1775 - mse: 0.1775 Epoch 64/200 1/1 [==============================] - 0s 3ms/step - loss: 0.1718 - mse: 0.1718 Epoch 65/200 1/1 [==============================] - 0s 2ms/step - loss: 0.1662 - mse: 0.1662 Epoch 66/200 1/1 [==============================] - 0s 2ms/step - loss: 0.1608 - mse: 0.1608 Epoch 67/200 1/1 [==============================] - 0s 4ms/step - loss: 0.1556 - mse: 0.1556 Epoch 68/200 1/1 [==============================] - 0s 4ms/step - loss: 0.1506 - mse: 0.1506 Epoch 69/200 1/1 [==============================] - 0s 3ms/step - loss: 0.1457 - mse: 0.1457 Epoch 70/200 1/1 [==============================] - 0s 2ms/step - loss: 0.1410 - mse: 0.1410 Epoch 71/200 1/1 [==============================] - 0s 2ms/step - loss: 0.1364 - mse: 0.1364 Epoch 72/200 1/1 [==============================] - 0s 2ms/step - loss: 0.1320 - mse: 0.1320 Epoch 73/200 1/1 [==============================] - 0s 7ms/step - loss: 0.1277 - mse: 0.1277 Epoch 74/200 1/1 [==============================] - 0s 5ms/step - loss: 0.1236 - mse: 0.1236 Epoch 75/200 1/1 [==============================] - 0s 9ms/step - loss: 0.1196 - mse: 0.1196 Epoch 76/200 1/1 [==============================] - 0s 5ms/step - loss: 0.1157 - mse: 0.1157 Epoch 77/200 1/1 [==============================] - 0s 2ms/step - loss: 0.1119 - mse: 0.1119 Epoch 78/200 1/1 [==============================] - 0s 2ms/step - loss: 0.1083 - mse: 0.1083 Epoch 79/200 1/1 [==============================] - 0s 7ms/step - loss: 0.1048 - mse: 0.1048 Epoch 80/200 1/1 [==============================] - 0s 3ms/step - loss: 0.1014 - mse: 0.1014 Epoch 81/200 1/1 [==============================] - 0s 5ms/step - loss: 0.0981 - mse: 0.0981 Epoch 82/200 1/1 [==============================] - 0s 6ms/step - loss: 0.0949 - mse: 0.0949 Epoch 83/200 1/1 [==============================] - 0s 9ms/step - loss: 0.0919 - mse: 0.0919 Epoch 84/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 Epoch 85/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0860 - mse: 0.0860 Epoch 86/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0832 - mse: 0.0832 Epoch 87/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0805 - mse: 0.0805 Epoch 88/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0779 - mse: 0.0779 Epoch 89/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0754 - mse: 0.0754 Epoch 90/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0729 - mse: 0.0729 Epoch 91/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0706 - mse: 0.0706 Epoch 92/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0683 - mse: 0.0683 Epoch 93/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0661 - mse: 0.0661 Epoch 94/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0639 - mse: 0.0639 Epoch 95/200 1/1 [==============================] - 0s 5ms/step - loss: 0.0619 - mse: 0.0619 Epoch 96/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0599 - mse: 0.0599 Epoch 97/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0579 - mse: 0.0579 Epoch 98/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0560 - mse: 0.0560 Epoch 99/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0542 - mse: 0.0542 Epoch 100/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0525 - mse: 0.0525 Epoch 101/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0508 - mse: 0.0508 Epoch 102/200 1/1 [==============================] - 0s 1ms/step - loss: 0.0491 - mse: 0.0491 Epoch 103/200 1/1 [==============================] - 0s 6ms/step - loss: 0.0475 - mse: 0.0475 Epoch 104/200 1/1 [==============================] - 0s 5ms/step - loss: 0.0460 - mse: 0.0460 Epoch 105/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0445 - mse: 0.0445 Epoch 106/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0431 - mse: 0.0431 Epoch 107/200 1/1 [==============================] - 0s 1ms/step - loss: 0.0417 - mse: 0.0417 Epoch 108/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0403 - mse: 0.0403 Epoch 109/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0390 - mse: 0.0390 Epoch 110/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0377 - mse: 0.0377 Epoch 111/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0365 - mse: 0.0365 Epoch 112/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0353 - mse: 0.0353 Epoch 113/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0342 - mse: 0.0342 Epoch 114/200 1/1 [==============================] - 0s 6ms/step - loss: 0.0331 - mse: 0.0331 Epoch 115/200 1/1 [==============================] - 0s 5ms/step - loss: 0.0320 - mse: 0.0320 Epoch 116/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0310 - mse: 0.0310 Epoch 117/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0300 - mse: 0.0300 Epoch 118/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0290 - mse: 0.0290 Epoch 119/200 1/1 [==============================] - 0s 5ms/step - loss: 0.0281 - mse: 0.0281 Epoch 120/200 1/1 [==============================] - 0s 1ms/step - loss: 0.0271 - mse: 0.0271 Epoch 121/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0263 - mse: 0.0263 Epoch 122/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0254 - mse: 0.0254 Epoch 123/200 1/1 [==============================] - 0s 6ms/step - loss: 0.0246 - mse: 0.0246 Epoch 124/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0238 - mse: 0.0238 Epoch 125/200 1/1 [==============================] - 0s 1ms/step - loss: 0.0230 - mse: 0.0230 Epoch 126/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0223 - mse: 0.0223 Epoch 127/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0216 - mse: 0.0216 Epoch 128/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0209 - mse: 0.0209 Epoch 129/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0202 - mse: 0.0202 Epoch 130/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 Epoch 131/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0189 - mse: 0.0189 Epoch 132/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0183 - mse: 0.0183 Epoch 133/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0177 - mse: 0.0177 Epoch 134/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0171 - mse: 0.0171 Epoch 135/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0166 - mse: 0.0166 Epoch 136/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0160 - mse: 0.0160 Epoch 137/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0155 - mse: 0.0155 Epoch 138/200 1/1 [==============================] - 0s 1ms/step - loss: 0.0150 - mse: 0.0150 Epoch 139/200 1/1 [==============================] - 0s 6ms/step - loss: 0.0145 - mse: 0.0145 Epoch 140/200 1/1 [==============================] - 0s 14ms/step - loss: 0.0140 - mse: 0.0140 Epoch 141/200 1/1 [==============================] - 0s 5ms/step - loss: 0.0136 - mse: 0.0136 Epoch 142/200 1/1 [==============================] - 0s 8ms/step - loss: 0.0131 - mse: 0.0131 Epoch 143/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0127 - mse: 0.0127 Epoch 144/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0123 - mse: 0.0123 Epoch 145/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0119 - mse: 0.0119 Epoch 146/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0115 - mse: 0.0115 Epoch 147/200 1/1 [==============================] - 0s 5ms/step - loss: 0.0112 - mse: 0.0112 Epoch 148/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 Epoch 149/200 1/1 [==============================] - 0s 7ms/step - loss: 0.0104 - mse: 0.0104 Epoch 150/200 1/1 [==============================] - 0s 5ms/step - loss: 0.0101 - mse: 0.0101 Epoch 151/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 0.0098 Epoch 152/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0095 - mse: 0.0095 Epoch 153/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0092 - mse: 0.0092 Epoch 154/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0089 - mse: 0.0089 Epoch 155/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0086 - mse: 0.0086 Epoch 156/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 Epoch 157/200 1/1 [==============================] - 0s 5ms/step - loss: 0.0080 - mse: 0.0080 Epoch 158/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0078 - mse: 0.0078 Epoch 159/200 1/1 [==============================] - 0s 1ms/step - loss: 0.0075 - mse: 0.0075 Epoch 160/200 1/1 [==============================] - 0s 5ms/step - loss: 0.0073 - mse: 0.0073 Epoch 161/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 Epoch 162/200 1/1 [==============================] - 0s 5ms/step - loss: 0.0068 - mse: 0.0068 Epoch 163/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 Epoch 164/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0064 - mse: 0.0064 Epoch 165/200 1/1 [==============================] - 0s 7ms/step - loss: 0.0062 - mse: 0.0062 Epoch 166/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0060 - mse: 0.0060 Epoch 167/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0058 - mse: 0.0058 Epoch 168/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 Epoch 169/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0054 - mse: 0.0054 Epoch 170/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 Epoch 171/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0051 - mse: 0.0051 Epoch 172/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0049 - mse: 0.0049 Epoch 173/200 1/1 [==============================] - 0s 1ms/step - loss: 0.0047 - mse: 0.0047 Epoch 174/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0046 - mse: 0.0046 Epoch 175/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0044 - mse: 0.0044 Epoch 176/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0043 - mse: 0.0043 Epoch 177/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0042 - mse: 0.0042 Epoch 178/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0040 Epoch 179/200 1/1 [==============================] - 0s 4ms/step - loss: 0.0039 - mse: 0.0039 Epoch 180/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0038 - mse: 0.0038 Epoch 181/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0036 - mse: 0.0036 Epoch 182/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0035 - mse: 0.0035 Epoch 183/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0034 - mse: 0.0034 Epoch 184/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0033 - mse: 0.0033 Epoch 185/200 1/1 [==============================] - 0s 1ms/step - loss: 0.0032 - mse: 0.0032 Epoch 186/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0031 - mse: 0.0031 Epoch 187/200 1/1 [==============================] - 0s 5ms/step - loss: 0.0030 - mse: 0.0030 Epoch 188/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0029 - mse: 0.0029 Epoch 189/200 1/1 [==============================] - 0s 1ms/step - loss: 0.0028 - mse: 0.0028 Epoch 190/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0027 - mse: 0.0027 Epoch 191/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0026 - mse: 0.0026 Epoch 192/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0025 - mse: 0.0025 Epoch 193/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0024 - mse: 0.0024 Epoch 194/200 1/1 [==============================] - 0s 1ms/step - loss: 0.0024 - mse: 0.0024 Epoch 195/200 1/1 [==============================] - 0s 1ms/step - loss: 0.0023 - mse: 0.0023 Epoch 196/200 1/1 [==============================] - 0s 1ms/step - loss: 0.0022 - mse: 0.0022 Epoch 197/200 1/1 [==============================] - 0s 2ms/step - loss: 0.0021 - mse: 0.0021 Epoch 198/200 1/1 [==============================] - 0s 8ms/step - loss: 0.0021 - mse: 0.0021 Epoch 199/200 1/1 [==============================] - 0s 3ms/step - loss: 0.0020 - mse: 0.0020 Epoch 200/200 1/1 [==============================] - 0s 1ms/step - loss: 0.0019 - mse: 0.0019 . &lt;tensorflow.python.keras.callbacks.History at 0x7f406a593950&gt; . Now that we have the model we can generate a SavedModel . export_dir = &#39;saved_model/1&#39; tf.saved_model.save(model, export_dir) . INFO:tensorflow:Assets written to: saved_model/1/assets . INFO:tensorflow:Assets written to: saved_model/1/assets . Convert the SavedModel to TFLite . converter = tf.lite.TFLiteConverter.from_saved_model(export_dir) tflite_model = converter.convert() . tflite_model_file = pathlib.Path(&#39;model.tflite&#39;) tflite_model_file.write_bytes(tflite_model) . 776 . Load TFLite model and allocate tensors . interpreter = tf.lite.Interpreter(model_content=tflite_model) interpreter.allocate_tensors() . Get input and output tensors . input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() . Test the Tensorflow Lite model on random input data . input_shape = input_details[0][&#39;shape&#39;] inputs, outputs = [], [] for _ in range(100): input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32) interpreter.set_tensor(input_details[0][&#39;index&#39;], input_data) interpreter.invoke() tflite_results = interpreter.get_tensor(output_details[0][&#39;index&#39;]) # Test the Tensorflow model on random input tf_results = model(tf.constant(input_data)) output_data = np.array(tf_results) inputs.append(input_data[0][0]) outputs.append(output_data[0][0]) . Visualize the model . plt.plot(inputs, outputs) plt.show() .",
            "url": "https://juanchoalric.github.io/ai_journey/deep-learning/jupyter/tensorflow/keras/machine_learning/2020/10/13/keras-model-how-to-save-it.html",
            "relUrl": "/deep-learning/jupyter/tensorflow/keras/machine_learning/2020/10/13/keras-model-how-to-save-it.html",
            "date": " • Oct 13, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Mushroom Clasifier",
            "content": "import pandas as pd import numpy as np import matplotlib as plt import tensorflow as tf import matplotlib.pyplot as plt . df = pd.read_csv(&quot;datasets/mushrooms.csv&quot;) . df.head() . class cap-shape cap-surface cap-color bruises odor gill-attachment gill-spacing gill-size gill-color ... stalk-surface-below-ring stalk-color-above-ring stalk-color-below-ring veil-type veil-color ring-number ring-type spore-print-color population habitat . 0 p | x | s | n | t | p | f | c | n | k | ... | s | w | w | p | w | o | p | k | s | u | . 1 e | x | s | y | t | a | f | c | b | k | ... | s | w | w | p | w | o | p | n | n | g | . 2 e | b | s | w | t | l | f | c | b | n | ... | s | w | w | p | w | o | p | n | n | m | . 3 p | x | y | w | t | p | f | c | n | n | ... | s | w | w | p | w | o | p | k | s | u | . 4 e | x | s | g | f | n | f | w | b | k | ... | s | w | w | p | w | o | e | n | a | g | . 5 rows × 23 columns . df.columns . Index([&#39;class&#39;, &#39;cap-shape&#39;, &#39;cap-surface&#39;, &#39;cap-color&#39;, &#39;bruises&#39;, &#39;odor&#39;, &#39;gill-attachment&#39;, &#39;gill-spacing&#39;, &#39;gill-size&#39;, &#39;gill-color&#39;, &#39;stalk-shape&#39;, &#39;stalk-root&#39;, &#39;stalk-surface-above-ring&#39;, &#39;stalk-surface-below-ring&#39;, &#39;stalk-color-above-ring&#39;, &#39;stalk-color-below-ring&#39;, &#39;veil-type&#39;, &#39;veil-color&#39;, &#39;ring-number&#39;, &#39;ring-type&#39;, &#39;spore-print-color&#39;, &#39;population&#39;, &#39;habitat&#39;], dtype=&#39;object&#39;) . df.shape . (8124, 23) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 8124 entries, 0 to 8123 Data columns (total 23 columns): # Column Non-Null Count Dtype -- -- 0 class 8124 non-null object 1 cap-shape 8124 non-null object 2 cap-surface 8124 non-null object 3 cap-color 8124 non-null object 4 bruises 8124 non-null object 5 odor 8124 non-null object 6 gill-attachment 8124 non-null object 7 gill-spacing 8124 non-null object 8 gill-size 8124 non-null object 9 gill-color 8124 non-null object 10 stalk-shape 8124 non-null object 11 stalk-root 8124 non-null object 12 stalk-surface-above-ring 8124 non-null object 13 stalk-surface-below-ring 8124 non-null object 14 stalk-color-above-ring 8124 non-null object 15 stalk-color-below-ring 8124 non-null object 16 veil-type 8124 non-null object 17 veil-color 8124 non-null object 18 ring-number 8124 non-null object 19 ring-type 8124 non-null object 20 spore-print-color 8124 non-null object 21 population 8124 non-null object 22 habitat 8124 non-null object dtypes: object(23) memory usage: 1.4+ MB . from sklearn.preprocessing import LabelBinarizer from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import train_test_split . lb = LabelBinarizer() o_hot = OneHotEncoder() . X = df.drop([&#39;class&#39;], axis=1).values . y = df[&#39;class&#39;].values . X_one_h = o_hot.fit_transform(X) . y_lb = lb.fit_transform(y) . X_train, X_test, y_train, y_test = train_test_split(X_one_h, y_lb, test_size=0.2, random_state=42) . X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.2, random_state=42) . X_train.shape . (5199, 117) . X_validate.shape . (1300, 117) . X_test.shape . (1625, 117) . tf.keras.backend.clear_session() model = tf.keras.Sequential([ tf.keras.layers.Dense(32, input_shape=(117,), activation=&#39;relu&#39;), tf.keras.layers.Dense(64, activation=&#39;relu&#39;), tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;) ]) . model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . history = model.fit( x=X_train, y=y_train, validation_data=(X_validate, y_validate), validation_steps=8, epochs=5 ) . Epoch 1/5 163/163 [==============================] - 0s 2ms/step - loss: 0.1996 - accuracy: 0.9283 - val_loss: 0.0333 - val_accuracy: 0.9923 Epoch 2/5 163/163 [==============================] - 0s 1ms/step - loss: 0.0103 - accuracy: 0.9987 - val_loss: 0.0049 - val_accuracy: 0.9992 Epoch 3/5 163/163 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 Epoch 4/5 163/163 [==============================] - 0s 2ms/step - loss: 8.3745e-04 - accuracy: 1.0000 - val_loss: 9.0848e-04 - val_accuracy: 1.0000 Epoch 5/5 163/163 [==============================] - 0s 1ms/step - loss: 4.5085e-04 - accuracy: 1.0000 - val_loss: 5.6778e-04 - val_accuracy: 1.0000 . acc = history.history[&#39;accuracy&#39;] val_acc = history.history[&#39;val_accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] epochs = range(len(acc)) . plt.plot(epochs, acc) plt.plot(epochs, val_acc) plt.figure() . &lt;Figure size 432x288 with 0 Axes&gt; . &lt;Figure size 432x288 with 0 Axes&gt; . plt.plot(epochs, loss) plt.plot(epochs, val_loss) plt.figure() . &lt;Figure size 432x288 with 0 Axes&gt; . &lt;Figure size 432x288 with 0 Axes&gt; . model.evaluate(X_test, y_test) . 51/51 [==============================] - 0s 2ms/step - loss: 4.9340e-04 - accuracy: 1.0000 . [0.0004933957825414836, 1.0] .",
            "url": "https://juanchoalric.github.io/ai_journey/deep-learning/jupyter/tensorflow/keras/machine_learning/2020/10/12/mushroom-classifier.html",
            "relUrl": "/deep-learning/jupyter/tensorflow/keras/machine_learning/2020/10/12/mushroom-classifier.html",
            "date": " • Oct 12, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "GD (Gradient Descent)",
            "content": "GD is the key that allow us to have a model that can get better and better and look for that perfection. For this we need a way to adjust the parameters so that we can get a better performance from each iteration. . We could look at each individual feature and come up with a set of parameters for each one, such that the highest parameters are associated with those features most likely to be important for a particular output. . This can be represented as a function and set of parameter values for each possible output instance the probability of being correct: . x= features . p=parameters . def pr_eight(x,p) = (x*p).sum() . x is represented as a vector, with all of the rows stacked up end to end into a single long line (x=[2,3,2,4,3,4,5,6,....,n]) and p is also a vector. If we have this function we only need a way of updating those &quot;p&quot; values until they are good as we can make them. . To be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier: . Initialize the weights. | For each feature, use these weights to predict the output. | Based on these predictions, calculate how good the model is (its loss). | Calculate the gradient, which measures for each weight, how changing that weight would change the loss | Step (that is, change) all the weights based on that calculation. | Go back to the step 2, and repeat the process. | Iterate until you decide to stop the training process (for instance, because the model is good enough or you don&#39;t want to wait any longer). | Initialize: We initialize the parameters to random values | Loss: testing the effectiveness of any current parameter assigment in terms of the actual performance. We need number that will return a small number if the performance was good or a large one if the performance was bad. | Step: A simple way to figure out whether a weight should be increased or decrease a bit. The best way to do this is by calculating the &quot;gradients&quot;. | Stop: Once you decided how many epochs (iterations) to train the model for, we apply that decision. Train until we ran out of time or the accuracy of the model starts to get worst. | . Simple case . from fastai.vision.all import * from fastbook import * . def f(x): return x**2 . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) plt.scatter(-1.5, f(-1.5), color=&#39;red&#39;); . If we decide to increment x just for a tiny value we can see that we would descend from the actual spot . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) plt.scatter(-1.5, f(-1.5), color=&#39;red&#39;, alpha=0.5); plt.scatter(-1, f(-1), color=&#39;red&#39;); . We try and get even lower . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) plt.scatter(-1.5, f(-1.5), color=&#39;red&#39;, alpha=0.5); plt.scatter(-1, f(-1), color=&#39;red&#39;, alpha=0.5); plt.scatter(-0.8, f(-0.8), color=&#39;red&#39;, alpha=0.5); plt.scatter(-0.7, f(-0.7), color=&#39;red&#39;); . Calculating the gradient . The &quot;one magic step&quot; is the bit where we calculate the gradients. As we mentioned, we use calculus as a performance optimization; it allows us to more quickly calculate whether our loss will go up or down when we adjust our parameters up or down. In other words, the gradients will tell us how much we have to change each weight to make our model better. . Did you study calculus in school? . If you remember that the derivative of a function tells you how much a change in its parameter will change its result. If not dont worry you just stop for a minute and go and watch this awesome video made by 3blue1brown . https://www.youtube.com/watch?v=9vKqVkMQHKk&amp;list=PL0-GT3co4r2wlh6UHTUeQsrf3mlS2lk6x&amp;index=2 . Now that you refresh about derivatives we can continue . Remember the function x^2? Well its derivative is another function that calculates the change, rather than de value. For instance, the derivative of x^2 at the value 5 tells us how rapidly the function changes at the value 5. When we know how our function will change, then we know what to do to make it smaller. This is the key to machine learning: having a way to change the parameters of a function to make it smaller. . One important thing to be aware of is that our function has lots of weights that we need to adjust, so when we calculate the derivative we won&#39;t get back one number, but lots of them a gradient for every weight. But there is nothing mathematically tricky here; you can calculate the derivative with respect to one weight, and treat all the other ones as constant, then repeat that for each other weight. This is how all of the gradients are calculated, for every weight. . Well... the best of all of this is that...PyTorch is able to automatically compute the derivative of nearly any function! and its surprinsingly fast . 1) Lests pick a tensor value which we want gradients at: . xt = tensor(5.).requires_grad_() . requiresgrad is a method brought to us by pytorch. We use it to tell Pytorch that we want to calculate grandients with respect to that variable at that specific value. This will make Pytorch remember to keep track of how to compute grandients of the other. . Now lets calculate the function with that specific value . yt = f(xt) yt . tensor(25., grad_fn=&lt;PowBackward0&gt;) . Finally we tell Pytorch to calculate the gradient for us . yt.backward() . xt.grad . tensor(10.) . If you remember your high school calculus rules, the derivative of x*2 is 2x, and we have x=3, so the gradients should be 2*5=10 which is what PyTorch calculated for us! . Lets now do it with a vector instead of only 1 number . xt = tensor([3., 5., 15.]).requires_grad_() xt . tensor([ 3., 5., 15.], requires_grad=True) . lets change the first function to add all those numbers in the vector . def f(x): return (x**2).sum() yt = f(xt) yt . tensor(259., grad_fn=&lt;SumBackward0&gt;) . yt.backward() xt.grad . tensor([ 6., 10., 30.]) . The gradients only tell us the slope of our function, they don&#39;t actually tell us exactly how far to adjust the parameters. But it gives us some idea of how far; if the slope is very large, then that may suggest that we have more adjustments to do, whereas if the slope is very small, that may suggest that we are close to the optimal value. . Adjusting using the Learning rate . Deciding how to modify our parameters based on the values of the gradients is a crusial part of the process of deep learning . We will multiply the gradient by some small number aka &quot;the learning rate (LR)&quot;. . Common pick numbers rank between 0.001 and 0.1. Once you have picked a LR, you can adjust your parameters using this simple funtion: . p -= gradient(p) * lr -&gt; this is known as stepping your parameters. . What happends if you pick a learning rate to small? . It can mean having to do a lot of steps :( . What happends if you pick a learning rate to high? . Well it could actually result in the loss getting worse and bouncing back up . Lets work on a end-to-end simple example . Lets take a look at GD and see how finding a minimum can be used to train a model to fit data better . Let&#39;s start with a simple model. . Imagine you were measuring the speed of a roller coaster as it went over the top of a hump. It would start fast, and then get slower as it went up the hill; it would be slowest at the top, and it would then speed up again as it went downhill. You want to build a model of how the speed changes over time. If you were measuring the speed manually every second for 60 seconds, it might look something like this: . time = torch.arange(0,20).float() . time . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) . speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1 . plt.scatter(time, speed); . Lets try and guess that is a &quot;quadratic function&quot; of the form: . a(time**2)+(btime)+c . def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . This greatly simplifies the problem, since every quadratic function is fully defined by the three parameters a, b, and c. Thus, to find the best quadratic function, we only need to find the best values for a, b, and c. . We need to define first what we mean by &quot;best.&quot; We define this precisely by choosing a loss function, which will return a value based on a prediction and a target, where lower values of the function correspond to &quot;better&quot; predictions. For continuous data, it&#39;s common to use mean squared error: . def mse(preds, targets): return ((preds-targets)**2).mean() . Now lets implement the 7 step process from the begining of the post . Step 1: Initialize the parameters . We are going to initialized each parameter with a random value and tell Pytorch that we want to track their gradients using _requiresgrad() . params = torch.randn(3).requires_grad_() . We can clone the original parameters to have them just in case . original_parameters = params.clone() . Step 2: Calculate the predictions . preds = f(time,params) . Lets see how the predictions are to our real targets . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) ax.set_ylim(-300,100) . show_preds(preds) . Wow! Terrible our random values think that the roller coster is going backwards... look at the negative speed . Can we do a better job? . Well, lets calculate the loss . Step 3: Calculate the loss . loss = mse(preds, speed) . loss . tensor(31688.4062, grad_fn=&lt;MeanBackward0&gt;) . Our goal is now to improve this. To do that, we&#39;ll need to know the gradients. . Step 4: Calculate the gradients . loss.backward() params.grad . tensor([-59061.2148, -3802.9961, -281.0778]) . We can now pick a learning rate to try and adjust this gradients . lr = 0.00001 params.data -= lr * params.grad.data params.grad = None . Lets see if the loss has improved: . preds = f(time,params) mse(preds, speed) . tensor(6555.9229, grad_fn=&lt;MeanBackward0&gt;) . show_preds(preds) . We need to repeat this a few times . def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . Step 6: Repeat the process . Now we repeat this process a bunch of times and see if we get any improvements . for i in range(20): apply_step(params) . 6555.9228515625 1800.084228515625 900.1290283203125 729.8250732421875 697.5931396484375 691.4888305664062 690.3287353515625 690.1041870117188 690.0565795898438 690.0423583984375 690.0346069335938 690.0282592773438 690.0219116210938 690.015625 690.0093383789062 690.0029907226562 689.9968872070312 689.9906005859375 689.9844360351562 689.9781494140625 . # this time with a graph params = original_parameters.detach().requires_grad_() . _,axs = plt.subplots(1,4,figsize=(12,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . Step 7: stop . Just decide a number of epochs and stop the process when you feel happy with the loss . I hope that this post help you undestand a bit more on why we use gradient Descent in machine learning and how powerful it can be. In my next post we will discuss this process in more detail using the Mnist dataset as example to show you GD apply into a real problem. .",
            "url": "https://juanchoalric.github.io/ai_journey/deep-learning/jupyter/fastai/2020/09/30/Gradient-descent-simple-example.html",
            "relUrl": "/deep-learning/jupyter/fastai/2020/09/30/Gradient-descent-simple-example.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Creating a Simpler Model",
            "content": "# your machine to run the code !pip install -Uqq fastbook import fastbook fastbook.setup_book() . voila 0.2.3 has requirement nbconvert&lt;7,&gt;=6.0.0, but you&#39;ll have nbconvert 5.6.1 which is incompatible. . /home/juancruzalric/.pyenv/versions/3.7.4/envs/fastai/lib/python3.7/site-packages/fastbook/__init__.py:22: UserWarning: Missing `graphviz` - please run `conda install fastbook` except ModuleNotFoundError: warn(&#34;Missing `graphviz` - please run `conda install fastbook`&#34;) . from fastai.vision.all import * from fastbook import * # We are going to work with grey scale images matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . Mnist Dataset . Mnist Dataset contains a lot of images from 0 to 9. We are going to try and classify any image as a 3 or a 7 . URLs.MNIST_SAMPLE . &#39;https://s3.amazonaws.com/fast-ai-sample/mnist_sample.tgz&#39; . path = untar_data(URLs.MNIST_SAMPLE) . path . Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample&#39;) . path.ls() . (#3) [Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/valid&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/labels.csv&#39;)] . ls() &gt; is a method created by fastai. It let you see what is inside the directory. It will return a object called by &quot;L&quot;. It has the same functionality as a &quot;list&quot; in python. . We can also see that we have a train directory and a valid directory . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/7&#39;)] . There are two directories inside &quot;/train&quot; one is &quot;/train/7&quot; and the other one is &quot;/train/3&quot;. The &quot;/3&quot; and &quot;/7&quot; represents the labels of the data . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() threes . (#6131) [Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10000.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10011.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10031.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10034.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10042.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10052.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/1007.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10074.png&#39;),Path(&#39;/home/juancruzalric/.fastai/data/mnist_sample/train/3/10091.png&#39;)...] . Lets try and print out one of the images . img3_path = threes[1] im3 = Image.open(img3_path) im3 . img7_path = sevens[1] im7 = Image.open(img7_path) im7 . But sadly... a computer only undestands numbers we will need to transform this images . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . We are printing rows from 4 up to 10 (not including 10) and columns 4 to 10 (not including 10) . We can see it more clearly using a Dataframe (Pandas library). . type(array(im3)) . numpy.ndarray . type(tensor(im3)) . torch.Tensor . tensors and numpy arrays are similar. However tensors let you run your operations in the GPU making your work process faster. We are going to use tensors from now on. . img3_t = tensor(im3) df = pd.DataFrame(img3_t[4:22, 4:22]) ## Just setting the font-size and a background color as Grey with some gradient to see the ## density of the pixels df.style.set_properties(**{&#39;font-size&#39;:&#39;10pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . 11 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 9 | 51 | 119 | 253 | 253 | 253 | 76 | 0 | . 12 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 183 | 253 | 253 | 139 | 0 | . 13 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 182 | 253 | 253 | 104 | 0 | . 14 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 85 | 249 | 253 | 253 | 36 | 0 | . 15 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 60 | 214 | 253 | 253 | 173 | 11 | 0 | . 16 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 98 | 247 | 253 | 253 | 226 | 9 | 0 | 0 | . 17 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 42 | 150 | 252 | 253 | 253 | 233 | 53 | 0 | 0 | 0 | . Creating a SIMPLE Model . We are going to create a simple model that takes the AVG (averages) of each class to predict and use that to check if a new image pixels are near the average of the 3 or the 7. . seven_tensors = [tensor(Image.open(path)) for path in sevens] three_tensors = [tensor(Image.open(path)) for path in threes] len(three_tensors), len(seven_tensors) . (6131, 6265) . This is getting each path for the corresponding images and transforming them into a tensor, however all those tensors are inside a list . type(seven_tensors) . list . type(seven_tensors[0]) . torch.Tensor . show_image(seven_tensors[5]); . show_image() is going to transform those tensors into a image because each value inside the tensor represent a pixel . We would like to compute the avg of the density of each pixel. We will neet to stack each image and create a 3 dimencional tensor, which the hight is going to be the amount of images, and the other 2 dimensions are going to be 28x28 pixels size. We can use the stack method given us by pytorch. . stacked_sevens = torch.stack(seven_tensors).float() / 255 stacked_threes = torch.stack(three_tensors).float() / 255 stacked_threes.shape . torch.Size([6131, 28, 28]) . we can also check the dimensions by calling ndim . stacked_threes.ndim . 3 . mean3 = stacked_threes.mean(0) show_image(mean3) . &lt;AxesSubplot:&gt; . mean7 = stacked_sevens.mean(0) show_image(mean7) . &lt;AxesSubplot:&gt; . Now we have the ideal digits for 3 and 7. Let us pick an arbitrary 3 and measure its distance to the ideal one. We are going to explain two different ways . Take the mean of the absolute value of differences (absolute value is the function that replaces negative values with positive values). This is called the mean absolute difference or L1 norm | Take the mean of the square of differences (which makes everything positive) and then take the square root (which undoes the squaring). This is called the root mean squared error (RMSE) or L2 norm. | . a_3 = stacked_threes[3] show_image(a_3); . dist_3_abs = (a_3 - mean3).abs().mean() dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt() dist_3_abs, dist_3_sqr . (tensor(0.1154), tensor(0.2064)) . dist_7_abs = (a_3 - mean7).abs().mean() dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt() dist_7_abs, dist_7_sqr . (tensor(0.1669), tensor(0.3170)) . We can see that the distance betweem the 3 and the &quot;ideal&quot; 3 is shorter than the &quot;ideal&quot; 7 pytorch already provides both of these as loss functions . F.l1_loss(a_3.float(), mean7), F.mse_loss(a_3, mean7).sqrt() . (tensor(0.1669), tensor(0.3170)) . The mean_square_error will penalize bigger mistakes more heavily and be more lenient with small mistakes . Metric . A metric is a number that is calculated based on the predictions of our model, and the correct labels in our dataset, in order to tell us how good our model is. Remeber that the Mnist dataset already has a directory called &quot;Valid&quot;. This directory contains the data where we are going to calculate how well our model operates. . We are going to use the &#39;stack&#39; pytorch operation to stack all the images contain in de valid directory . valid_3_tens = torch.stack([tensor(Image.open(path)) for path in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(path)) for path in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Here we see two tensors, one representing the 3s validation set of 1,010 images of size 28×28, and one representing the 7s validation set of 1,028 images of size 28×28. . We can write a simple function that calculates the mean absolute error using an experssion very similar to the one we wrote in the last section: . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) . valid_3_dist = mnist_distance(valid_3_tens, mean3) valid_3_dist, valid_3_dist.shape . (tensor([0.1605, 0.1107, 0.1181, ..., 0.1419, 0.1103, 0.1162]), torch.Size([1010])) . We can use mnist_distance to figure out whether an image is a 3 or not by using the following logic: if the distance between the digit in question and the ideal 3 is less than the distance to the ideal 7, then it&#39;s a 3. This function will automatically do broadcasting and be applied elementwise, just like all PyTorch functions and operators: . def is_3(x): return mnist_distance(x, mean3) &lt; mnist_distance(x, mean7) . Let&#39;s test it with one of our examples . is_3(a_3), is_3(a_3).float() . (tensor(True), tensor(1.)) . is_3(valid_3_tens) . tensor([False, True, True, ..., True, True, True]) . Now we can calculate the accuracy for each of the 3s and 7s by taking the average of that function for all 3s and its inverse for all 7s: . accuracy_3s = is_3(valid_3_tens).float().mean() accuracy_7s = (1- is_3(valid_7_tens).float()).mean() accuracy_3s, accuracy_7s, (accuracy_3s+accuracy_7s)/2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) . We&#39;re getting over 90% accuracy on both 3s and 7s, and we&#39;ve seen how to define a metric conveniently using broadcasting. . However: 3s and 7s are very different-looking digits. And we&#39;re only classifying 2 out of the 10 possible digits so far. So we&#39;re going to need to do better! . But now we know a simpler model that can predict with a 95% accuracy and if we create a deep learning model that predict with 94% accuracy we can clearly see that is not a good model and we will need to try other things. .",
            "url": "https://juanchoalric.github.io/ai_journey/deep-learning/jupyter/fastai/2020/09/28/creating-simple-model.html",
            "relUrl": "/deep-learning/jupyter/fastai/2020/09/28/creating-simple-model.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastai First Chapter",
            "content": "Why is it hard to use a traditional computer program to recognize images in a photo? . We are going to answer this question by giving an example: Imagine that you make a program that tells you if the image that you pass is an apple . . But not every apple has the same size and color... . . Image creating a bunch of manual rules for only one image just to get rekt by the second image they give you. . And that&#39;s why machine learning is perfect for solving image detection problems, just let the computer create her own rules that work for any apple image. . What did the author mean by &quot;weight assignment&quot;? . Weight assignments are a specific way to add values to the current weights. In deep learning we use Parameters to refer to &quot;weights&quot; . What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy? . The universal approximation theorem shows that this function can solve any problem to any level of accuracy, in theory. . Neural network diagram . . What do you need in order to train a model? . You need DATA. If you are working in a supervise problem you also need the labels for that particula dataset . Do we always have to use 224&#215;224-pixel images with the cat recognition model? . No, 224x224 is a well establish size known by researchers, but you can use the size you want. Just take into account that if you increase the size you may get better predictions because the model can track more data. However, you will lose speed in training your model. . What is the difference between classification and regression? . The difference between classification and regression is the target you want to predict. In REGRESSION problems you want to predict a number (ex:123). In CLASSIFICATION problems you want to predict a specific class (ex: if its a dog or a cat) . What is a validation set? What is a test set? Why do we need them? . A validation set is used to tell if our model is performing better in each iteration in the training face. The test set is only used when we are completely sure that we are going to use that model and we are ready to deploy de model to production. . What will fastai do if you don&#39;t provide a validation set? . It will create a validation set for you . What is overfitting? Provide an example. . Overfitting is when your model memorizes all your training set so well that when given new unseen data it gives a terrible result. Example: Image a teacher tells you to only study from these questions for the exam. However, when the day of the exam arrives the test is completely different. You study only the questions by hard but you only can answer exactly those. If you are given another set of questions you would perform poorly. . What is a metric? How does it differ from &quot;loss&quot;? . A metric is a measurement of how good the model is, using the validation set, chosen for human perception. However, the loss is also a measure of how good the model is, chosen to drive training via SGD and in charge of modifying the parameters. . How can pretrained models help? . Transfer learning models help us because they already have their weights already initialized. It will also reduce the amount of time we are going to need our model to train and also the amount of data that we need to have. . What is the &quot;head&quot; of a model? . The head of a model is the part that is newly added to be specific to the new dataset. . What is an &quot;architecture&quot;? . An &quot;architecture&quot; is the actual mathematical function that we&#39;re passing the input data and parameters to . What is segmentation? . Creating a model that can recognize the content of every individual pixel in an image is called segmentation. . What is y_range used for? When do we need it? . Y_range is used when you&#39;re predicting a continuous number, rather than a category, so we have to tell fastai what range our target has, using the y_range parameter. . Summary . The first chapter was a perfect and clear introduction to a lot of words that we use all-day in Data Science. Hopefully, in the following chapters, we can get more deeply into these subjects. .",
            "url": "https://juanchoalric.github.io/ai_journey/deep-learning/jupyter/fastai/2020/09/26/fastai-first-questioner.html",
            "relUrl": "/deep-learning/jupyter/fastai/2020/09/26/fastai-first-questioner.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://juanchoalric.github.io/ai_journey/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://juanchoalric.github.io/ai_journey/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://juanchoalric.github.io/ai_journey/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}